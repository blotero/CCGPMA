\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{zhang2019crowdsourced}
\citation{liu2020truth}
\citation{kara2015modeling}
\citation{raykar2010learning}
\citation{zhu2019unsupervised}
\citation{snow2008cheap}
\citation{tao2018domain,wang2016bi}
\citation{groot2011learning}
\citation{rizos2020average}
\citation{morales2019scalable}
\citation{zhang2014imbalanced}
\citation{kara2015modeling}
\citation{dawid1979maximum}
\citation{zhang2014imbalanced}
\citation{ruiz2019learning}
\citation{raykar2010learning}
\citation{rodrigues2014gaussian,ruiz2019learning}
\citation{morales2019scalable,gonzalez2015automatic}
\citation{groot2011learning,rodrigues2017learning}
\citation{rodrigues2014sequence}
\citation{albarqouni2016aggnet,rodrigues2018deep,guan2018said}
\@LN@col{1}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\@LN@col{2}
\citation{g2019machine}
\citation{rodrigues2013learning}
\citation{gonzalez2015automatic}
\citation{raykar2010learning}
\citation{rodrigues2017learning}
\citation{rodrigues2014gaussian}
\citation{morales2019scalable}
\citation{raykar2010learning}
\citation{venanzi2014community}
\citation{tang2019leveraging}
\citation{zhang2011learning}
\citation{surowiecki2005wisdom,hahn2018communication}
\citation{zhu2019unsupervised}
\citation{g2019machine}
\citation{saul2016chained}
\citation{alvarez2012kernels}
\citation{teh2005semiparametric}
\citation{alvarez2010efficient}
\citation{hoffman2013stochastic}
\citation{g2019machine}
\citation{rodrigues2013learning}
\citation{gonzalez2015automatic}
\citation{raykar2010learning}
\citation{rodrigues2017learning}
\citation{rodrigues2014gaussian}
\citation{morales2019scalable}
\citation{raykar2010learning}
\citation{zhang2011learning}
\citation{yan2014learning}
\citation{wang2016bi}
\citation{xiao2013learning}
\citation{bishop2006pattern}
\@LN@col{1}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related work and main contributions}{2}{section.2}\protected@file@percent }
\citation{venanzi2014community}
\citation{tang2019leveraging}
\citation{zhang2011learning}
\citation{surowiecki2005wisdom,hahn2018communication}
\citation{g2019machine}
\citation{zhu2019unsupervised}
\citation{gil2018learning}
\citation{rodrigues2014gaussian,groot2011learning,ruiz2019learning,morales2019scalable,morales2019scalable1}
\citation{rodrigues2014gaussian,groot2011learning}
\citation{ruiz2019learning,morales2019scalable,morales2019scalable1}
\citation{yan2014learning,xiao2013learning}
\citation{zhu2019unsupervised,gil2018learning}
\citation{rodrigues2017learning,morales2019scalable,ruiz2019learning}
\citation{kara2015modeling}
\citation{raykar2010learning}
\citation{zhang2011learning}
\citation{xiao2013learning}
\citation{yan2014learning}
\citation{wang2016bi}
\citation{rodrigues2017learning}
\citation{gil2018learning}
\citation{hua2018collaborative}
\citation{ruiz2019learning}
\citation{morales2019scalable}
\citation{zhu2019unsupervised}
\citation{raykar2010learning}
\@LN@col{1}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {III}Methods}{3}{section.3}\protected@file@percent }
\newlabel{sec:methods}{{III}{3}{Methods}{section.3}{}}
\newlabel{sec:methods@cref}{{[section][3][]III}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Supervised learning from multiple annotators}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Chained Gaussian Processes for multiple annotators--(CGPMA)}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:CGP}{{\mbox  {III-B}}{3}{Chained Gaussian Processes for multiple annotators--(CGPMA)}{subsection.3.2}{}}
\newlabel{sec:CGP@cref}{{[subsection][2][3]\mbox  {III-B}}{[1][3][]3}}
\newlabel{eq:Likelihood}{{1}{3}{Chained Gaussian Processes for multiple annotators--(CGPMA)}{equation.3.1}{}}
\newlabel{eq:Likelihood@cref}{{[equation][1][]1}{[1][3][]3}}
\citation{hensman2015scalable}
\citation{saul2016chained,moreno2018heterogeneous}
\citation{hensman2015scalable,saul2016chained}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Survey of relevant supervised learning models devoted to multiple annotators.}}{4}{table.1}\protected@file@percent }
\newlabel{tab:SOA}{{I}{4}{Survey of relevant supervised learning models devoted to multiple annotators}{table.1}{}}
\newlabel{tab:SOA@cref}{{[table][1][]I}{[1][3][]4}}
\@LN@col{1}
\newlabel{eq:FgU}{{3}{4}{Chained Gaussian Processes for multiple annotators--(CGPMA)}{equation.3.3}{}}
\newlabel{eq:FgU@cref}{{[equation][3][]3}{[1][4][]4}}
\newlabel{eq:U}{{4}{4}{Chained Gaussian Processes for multiple annotators--(CGPMA)}{equation.3.4}{}}
\newlabel{eq:U@cref}{{[equation][4][]4}{[1][4][]4}}
\newlabel{eq:CGPpri}{{5}{4}{Chained Gaussian Processes for multiple annotators--(CGPMA)}{equation.3.5}{}}
\newlabel{eq:CGPpri@cref}{{[equation][5][]5}{[1][4][]4}}
\@LN@col{2}
\newlabel{eq:QU}{{7}{4}{Chained Gaussian Processes for multiple annotators--(CGPMA)}{equation.3.7}{}}
\newlabel{eq:QU@cref}{{[equation][7][]7}{[1][4][]4}}
\newlabel{eq:LowBound}{{8}{4}{Chained Gaussian Processes for multiple annotators--(CGPMA)}{equation.3.8}{}}
\newlabel{eq:LowBound@cref}{{[equation][8][]8}{[1][4][]4}}
\newlabel{eq:LowBound21}{{9}{4}{Chained Gaussian Processes for multiple annotators--(CGPMA)}{equation.3.9}{}}
\newlabel{eq:LowBound21@cref}{{[equation][9][]9}{[1][4][]4}}
\newlabel{eq:PostVar}{{10}{4}{Chained Gaussian Processes for multiple annotators--(CGPMA)}{equation.3.10}{}}
\newlabel{eq:PostVar@cref}{{[equation][10][]10}{[1][4][]4}}
\citation{teh2005semiparametric}
\citation{blei2017variational}
\citation{hensman2015scalable,saul2016chained,moreno2018heterogeneous}
\citation{moreno2018heterogeneous}
\citation{raykar2010learning,groot2011learning,xiao2013learning,rodrigues2017learning}
\citation{rodrigues2013learning}
\@LN@col{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Correlated Chained Gaussian Processes fro multiple annotators-(CCGPMA)}{5}{subsection.3.3}\protected@file@percent }
\newlabel{eq:SLFM}{{12}{5}{Correlated Chained Gaussian Processes fro multiple annotators-(CCGPMA)}{equation.3.12}{}}
\newlabel{eq:SLFM@cref}{{[equation][12][]12}{[1][5][]5}}
\newlabel{eq:CCGPpri}{{14}{5}{Correlated Chained Gaussian Processes fro multiple annotators-(CCGPMA)}{equation.3.14}{}}
\newlabel{eq:CCGPpri@cref}{{[equation][14][]14}{[1][5][]5}}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}CGPMA and CCGPMA applied to classification and regression tasks}{5}{subsection.3.4}\protected@file@percent }
\newlabel{eq:RegLik}{{19}{5}{CGPMA and CCGPMA applied to classification and regression tasks}{equation.3.19}{}}
\newlabel{eq:RegLik@cref}{{[equation][19][]19}{[1][5][]5}}
\newlabel{eq:ClasLik}{{20}{5}{CGPMA and CCGPMA applied to classification and regression tasks}{equation.3.20}{}}
\newlabel{eq:ClasLik@cref}{{[equation][20][]20}{[1][5][]5}}
\citation{ruiz2019learning}
\citation{rodrigues2013learning}
\citation{maaten2008visualizing}
\@LN@col{1}
\newlabel{fig:GMa}{{1(a)}{6}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{fig:GMa@cref}{{[subfigure][1][1]1(a)}{[1][6][]6}}
\newlabel{sub@fig:GMa}{{(a)}{6}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{fig:GMb}{{1(b)}{6}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{fig:GMb@cref}{{[subfigure][2][1]1(b)}{[1][6][]6}}
\newlabel{sub@fig:GMb}{{(b)}{6}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graphical plates for regression and classification models (see \cref  {eq:RegLik} and \cref  {eq:ClasLik}). Shaded nodes represent observed variables, and unshaded nodes indicate latent ones.}}{6}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { Regression}}}{6}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { Classification }}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:GM}{{1}{6}{Graphical plates for regression and classification models (see \cref {eq:RegLik} and \cref {eq:ClasLik}). Shaded nodes represent observed variables, and unshaded nodes indicate latent ones}{figure.1}{}}
\newlabel{fig:GM@cref}{{[figure][1][]1}{[1][6][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Set-up}{6}{section.4}\protected@file@percent }
\newlabel{sec:expsetup}{{IV}{6}{Experimental Set-up}{section.4}{}}
\newlabel{sec:expsetup@cref}{{[section][4][]IV}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Datasets and simulated/provided annotations}{6}{subsection.4.1}\protected@file@percent }
\newlabel{sec:datasets}{{\mbox  {IV-A}}{6}{Datasets and simulated/provided annotations}{subsection.4.1}{}}
\newlabel{sec:datasets@cref}{{[subsection][1][4]\mbox  {IV-A}}{[1][6][]6}}
\newlabel{sec:datasetsReg}{{\mbox  {IV-A}1}{6}{Regression}{subsubsection.4.1.1}{}}
\newlabel{sec:datasetsReg@cref}{{[subsubsection][1][4,1]\mbox  {IV-A}1}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}1}Regression}{6}{subsubsection.4.1.1}\protected@file@percent }
\@LN@col{2}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Datasets for regression. }}{6}{table.2}\protected@file@percent }
\newlabel{tab:RegData}{{II}{6}{Datasets for regression}{table.2}{}}
\newlabel{tab:RegData@cref}{{[table][2][]II}{[1][6][]6}}
\newlabel{sec:datasetsCla}{{\mbox  {IV-A}2}{6}{Classification}{subsubsection.4.1.2}{}}
\newlabel{sec:datasetsCla@cref}{{[subsubsection][2][4,1]\mbox  {IV-A}2}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-A}2}Classification}{6}{subsubsection.4.1.2}\protected@file@percent }
\citation{ruiz2019learning}
\citation{hernandez2020bearing}
\citation{arias2011automatic}
\citation{gil2018learning}
\citation{gonzalez2015automatic}
\citation{maaten2008visualizing}
\citation{raykar2010learning}
\citation{rodrigues2014gaussian}
\citation{rodrigues2018deep}
\citation{rodrigues2018deep}
\@LN@col{1}
\@LN@col{2}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Datasets used for classification. }}{7}{table.3}\protected@file@percent }
\newlabel{tab:ClaData}{{III}{7}{Datasets used for classification}{table.3}{}}
\newlabel{tab:ClaData@cref}{{[table][3][]III}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}CGPMA and CCGPMA training}{7}{subsection.4.2}\protected@file@percent }
\newlabel{sec:training}{{\mbox  {IV-B}}{7}{CGPMA and CCGPMA training}{subsection.4.2}{}}
\newlabel{sec:training@cref}{{[subsection][2][4]\mbox  {IV-B}}{[1][7][]7}}
\newlabel{eq:RBF}{{22}{7}{CGPMA and CCGPMA training}{equation.4.22}{}}
\newlabel{eq:RBF@cref}{{[equation][22][]22}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}Method comparison and performance metrics}{7}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-C}1}Regression}{7}{table.4}\protected@file@percent }
\citation{fawcett2006introduction}
\citation{raykar2010learning}
\citation{rodrigues2013learning}
\citation{rodrigues2014gaussian}
\citation{morales2019scalable}
\citation{rodrigues2018deep}
\citation{gil2018learning}
\citation{schmidhuber2015deep}
\citation{morales2019scalable1}
\citation{zhu2019unsupervised}
\@LN@col{1}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces A brief overview of state-of-the-art methods tested for regression tasks. GPR: Gaussian Processes Regression, LR: logistic regression, Av: average, MA: multiple annotators, DL: Deep learning, LFCR: Learning from crowds for regression. }}{8}{table.4}\protected@file@percent }
\newlabel{tab:RegVal}{{IV}{8}{A brief overview of state-of-the-art methods tested for regression tasks. GPR: Gaussian Processes Regression, LR: logistic regression, Av: average, MA: multiple annotators, DL: Deep learning, LFCR: Learning from crowds for regression}{table.4}{}}
\newlabel{tab:RegVal@cref}{{[table][4][]IV}{[1][7][]8}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces A brief overview of the state-of-the-art methods tested. GPC: Gaussian Processes classifier, LRC: logistic regression classifier, MV: majority voting, MA: multiple annotators, MAE: Modelling annotators expertise, LFC: Learning from crowds, DGRL: Distinguishing good from random labelers, KAAR: kernel alignment-based annotator relevance analysis. }}{8}{table.5}\protected@file@percent }
\newlabel{tab:ClaVal}{{V}{8}{A brief overview of the state-of-the-art methods tested. GPC: Gaussian Processes classifier, LRC: logistic regression classifier, MV: majority voting, MA: multiple annotators, MAE: Modelling annotators expertise, LFC: Learning from crowds, DGRL: Distinguishing good from random labelers, KAAR: kernel alignment-based annotator relevance analysis}{table.5}{}}
\newlabel{tab:ClaVal@cref}{{[table][5][]V}{[1][8][]8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-C}2}Classification}{8}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Results and Discussion}{8}{section.5}\protected@file@percent }
\@LN@col{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Regression}{8}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}1}fully synthetic data}{8}{subsubsection.5.1.1}\protected@file@percent }
\newlabel{eq:u1r}{{23}{8}{fully synthetic data}{equation.5.23}{}}
\newlabel{eq:u1r@cref}{{[equation][23][]23}{[1][8][]8}}
\newlabel{eq:u2r}{{24}{8}{fully synthetic data}{equation.5.24}{}}
\newlabel{eq:u2r@cref}{{[equation][24][]24}{[1][8][]8}}
\newlabel{eq:u3r}{{25}{8}{fully synthetic data}{equation.5.25}{}}
\newlabel{eq:u3r@cref}{{[equation][25][]25}{[1][8][]8}}
\newlabel{eq:parametersP}{{26}{8}{fully synthetic data}{equation.5.26}{}}
\newlabel{eq:parametersP@cref}{{[equation][26][]26}{[1][8][]8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Fully synthetic dataset results. We compare the prediction of our CCGPMA-R($R^2=0.9438$), and CCGPMA-R($R^2=0.9280$) with the theoretical upper bound GPR-GOLD($R^2=0.9843$) and lower bound GPR-Av($R^2=0.8718$), and state-of-the-art approaches, MA-LFCR($R^2=-0.0245$), MA-GPR($R^2=0.9208$), MA-DL-B($R^2=0.7020$), MA-DL-S($R^2=0.6559$), MA-DL-B+S($R^2=0.5997$). Note that we provided the Gold Standard in dashed lines. The shaded region in GPR-Av, MA-GPR, CGPMA-R, and CCGPMA-R indicates the area enclosed by the mean plus or minus two standard deviations. We remark that there is no shaded region for MA-LFCR, and DLMA since these approaches do not provide information about the prediction uncertainty.}}{9}{figure.2}\protected@file@percent }
\newlabel{fig:FSReg}{{2}{9}{Fully synthetic dataset results. We compare the prediction of our CCGPMA-R($R^2=0.9438$), and CCGPMA-R($R^2=0.9280$) with the theoretical upper bound GPR-GOLD($R^2=0.9843$) and lower bound GPR-Av($R^2=0.8718$), and state-of-the-art approaches, MA-LFCR($R^2=-0.0245$), MA-GPR($R^2=0.9208$), MA-DL-B($R^2=0.7020$), MA-DL-S($R^2=0.6559$), MA-DL-B+S($R^2=0.5997$). Note that we provided the Gold Standard in dashed lines. The shaded region in GPR-Av, MA-GPR, CGPMA-R, and CCGPMA-R indicates the area enclosed by the mean plus or minus two standard deviations. We remark that there is no shaded region for MA-LFCR, and DLMA since these approaches do not provide information about the prediction uncertainty}{figure.2}{}}
\newlabel{fig:FSReg@cref}{{[figure][2][]2}{[1][8][]9}}
\@LN@col{1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}2}Semi-synthetic data results}{9}{table.6}\protected@file@percent }
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Estimated values of error-variance for the five annotators in the \textit  {fully synthetic} experiment. In the first column, from top to bottom, we expose the error-variances $\mathbf  {v}_r$ used to simulate the labels from each annotator. Furthermore, the subsequent columns from top to bottom present the estimation of such error-variances performed by state-of-the-art models that include these kinds of parameters in their formulation; moreover, the true error-variances are provided in dashed lines. The shaded region in CGPMA-R and CCGPMA-R indicates the area enclosed by the mean plus or minus two standard deviations. We remark that there is no shaded region for MA-LFCR, and MA-GPR since these approaches perform a fixed-point estimation for the annotators' parameters. Finally, we remark that the $R^2$ score between the true and estimated error variances are provided.}}{10}{figure.3}\protected@file@percent }
\newlabel{fig:ExpReg}{{3}{10}{Estimated values of error-variance for the five annotators in the \textit {fully synthetic} experiment. In the first column, from top to bottom, we expose the error-variances $\boldv _r$ used to simulate the labels from each annotator. Furthermore, the subsequent columns from top to bottom present the estimation of such error-variances performed by state-of-the-art models that include these kinds of parameters in their formulation; moreover, the true error-variances are provided in dashed lines. The shaded region in CGPMA-R and CCGPMA-R indicates the area enclosed by the mean plus or minus two standard deviations. We remark that there is no shaded region for MA-LFCR, and MA-GPR since these approaches perform a fixed-point estimation for the annotators' parameters. Finally, we remark that the $R^2$ score between the true and estimated error variances are provided}{figure.3}{}}
\newlabel{fig:ExpReg@cref}{{[figure][3][]3}{[1][9][]10}}
\@LN@col{1}
\@LN@col{2}
\citation{rodrigues2017learning,rodrigues2018deep}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Regression results in terms of $R^2$ score over \textit  {semi synthetic datasets}. Bold: the highest $R^2$ excluding the upper bound GPR-GOLD.}}{11}{table.6}\protected@file@percent }
\newlabel{tab:SSRegResults}{{VI}{11}{Regression results in terms of $R^2$ score over \textit {semi synthetic datasets}. Bold: the highest $R^2$ excluding the upper bound GPR-GOLD}{table.6}{}}
\newlabel{tab:SSRegResults@cref}{{[table][6][]VI}{[1][9][]11}}
\@LN@col{1}
\newlabel{eq:Pkernel}{{27}{11}{Semi-synthetic data results}{equation.5.27}{}}
\newlabel{eq:Pkernel@cref}{{[equation][27][]27}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}3}fully real data}{11}{subsubsection.5.1.3}\protected@file@percent }
\@LN@col{2}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Regression results in terms of $R^2$ score over \textit  {fully real dataset}. Bold: the highest $R^2$ excluding the upper bound GPR-GOLD.}}{11}{table.7}\protected@file@percent }
\newlabel{tab:FSRegResults}{{VII}{11}{Regression results in terms of $R^2$ score over \textit {fully real dataset}. Bold: the highest $R^2$ excluding the upper bound GPR-GOLD}{table.7}{}}
\newlabel{tab:FSRegResults@cref}{{[table][7][]VII}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-B}}Classification}{11}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}1}Fully synthetic data}{11}{subsubsection.5.2.1}\protected@file@percent }
\citation{gil2018learning,rodrigues2014gaussian,ruiz2019learning}
\citation{bishop2006pattern}
\citation{zhu2019unsupervised,gil2018learning}
\citation{morales2019scalable1}
\@LN@col{1}
\newlabel{eq:u1c}{{28}{12}{Fully synthetic data}{equation.5.28}{}}
\newlabel{eq:u1c@cref}{{[equation][28][]28}{[1][11][]12}}
\newlabel{eq:u2c}{{29}{12}{Fully synthetic data}{equation.5.29}{}}
\newlabel{eq:u2c@cref}{{[equation][29][]29}{[1][11][]12}}
\newlabel{eq:u3c}{{30}{12}{Fully synthetic data}{equation.5.30}{}}
\newlabel{eq:u3c@cref}{{[equation][30][]30}{[1][11][]12}}
\newlabel{eq:parametersPC}{{31}{12}{Fully synthetic data}{equation.5.31}{}}
\newlabel{eq:parametersPC@cref}{{[equation][31][]31}{[1][12][]12}}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}2}Semi-synthetic data results}{12}{table.8}\protected@file@percent }
\citation{gonzalez2015automatic}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Fully synthetic dataset results (blue=class 1, red=class 2, and green=class3). We compare the prediction of our CCGPMA-C($AUC=1$), and CCGPMA-C($AUC=0.9999$) with the theoretical upper bound GPC-GOLD($AUC=1.0$) and lower bound GPC-MV($AUC=0.9809$), and state-of-the-art approaches, MA-LFC-C($AUC=0.9993$), MA-DGRL($AUC=0.9999$), MA-GPC($AUC=0.9977$), MA-GPCV($AUC=0.9515$), MA-DL-MW($AUC=0.9989$), MA-DL-VW($AUC=0.9972$), MA-DL-VW+B($AUC=0.9994$), KAAR($0.9099$). Note that the shaded region in GPC-MV, CGPMA-C, and CCGPMA-C indicates the area enclosed by the mean plus or minus two standard deviations. We remark that there is no shaded region for the rest of approaches since they do not provide information about the prediction uncertainty.}}{13}{figure.4}\protected@file@percent }
\newlabel{fig:FSCla}{{4}{13}{Fully synthetic dataset results (blue=class 1, red=class 2, and green=class3). We compare the prediction of our CCGPMA-C($AUC=1$), and CCGPMA-C($AUC=0.9999$) with the theoretical upper bound GPC-GOLD($AUC=1.0$) and lower bound GPC-MV($AUC=0.9809$), and state-of-the-art approaches, MA-LFC-C($AUC=0.9993$), MA-DGRL($AUC=0.9999$), MA-GPC($AUC=0.9977$), MA-GPCV($AUC=0.9515$), MA-DL-MW($AUC=0.9989$), MA-DL-VW($AUC=0.9972$), MA-DL-VW+B($AUC=0.9994$), KAAR($0.9099$). Note that the shaded region in GPC-MV, CGPMA-C, and CCGPMA-C indicates the area enclosed by the mean plus or minus two standard deviations. We remark that there is no shaded region for the rest of approaches since they do not provide information about the prediction uncertainty}{figure.4}{}}
\newlabel{fig:FSCla@cref}{{[figure][4][]4}{[1][12][]13}}
\@LN@col{1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}3}fully real data}{13}{subsubsection.5.2.3}\protected@file@percent }
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Estimated reliabilities for the five annotators in the. In the first column, from top to bottom, we expose the true reliabilities $\lambda _r$ used to simulate the labels from each annotator. The subsequent columns from top to bottom present the estimation of such reliabilities performed by state-of-the-art models that include these kinds of parameters in their formulation, where true values are provided in dashed lines. The shaded region in CGPMA-C and CCGPMA-C indicates the area enclosed by the mean plus or minus two standard deviations. Finally, we remark that the accuracy (Acc) is provided.}}{14}{figure.5}\protected@file@percent }
\newlabel{fig:ExpCla}{{5}{14}{Estimated reliabilities for the five annotators in the. In the first column, from top to bottom, we expose the true reliabilities $\lambda _r$ used to simulate the labels from each annotator. The subsequent columns from top to bottom present the estimation of such reliabilities performed by state-of-the-art models that include these kinds of parameters in their formulation, where true values are provided in dashed lines. The shaded region in CGPMA-C and CCGPMA-C indicates the area enclosed by the mean plus or minus two standard deviations. Finally, we remark that the accuracy (Acc) is provided}{figure.5}{}}
\newlabel{fig:ExpCla@cref}{{[figure][5][]5}{[1][12][]14}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces Regression results in terms of AUC score over \textit  {semi synthetic datasets}. Bold: the highest AUC excluding the upper bound GPC-GOLD.}}{14}{table.8}\protected@file@percent }
\newlabel{tab:SSClaResults}{{VIII}{14}{Regression results in terms of AUC score over \textit {semi synthetic datasets}. Bold: the highest AUC excluding the upper bound GPC-GOLD}{table.8}{}}
\newlabel{tab:SSClaResults@cref}{{[table][8][]VIII}{[1][12][]14}}
\@LN@col{1}
\@LN@col{2}
\citation{alvarez2011computationally}
\bibstyle{IEEEtran}
\bibdata{refs}
\bibcite{zhang2019crowdsourced}{{1}{}{{}}{{}}}
\bibcite{liu2020truth}{{2}{}{{}}{{}}}
\bibcite{kara2015modeling}{{3}{}{{}}{{}}}
\bibcite{raykar2010learning}{{4}{}{{}}{{}}}
\bibcite{zhu2019unsupervised}{{5}{}{{}}{{}}}
\bibcite{snow2008cheap}{{6}{}{{}}{{}}}
\bibcite{tao2018domain}{{7}{}{{}}{{}}}
\bibcite{wang2016bi}{{8}{}{{}}{{}}}
\bibcite{groot2011learning}{{9}{}{{}}{{}}}
\bibcite{rizos2020average}{{10}{}{{}}{{}}}
\bibcite{morales2019scalable}{{11}{}{{}}{{}}}
\@LN@col{1}
\@writefile{lot}{\contentsline {table}{\numberline {IX}{\ignorespaces Fully real datasets results. Bold: the method with the highest performance excluding the upper bound (target) classifier GPC-GOLD.}}{15}{table.9}\protected@file@percent }
\newlabel{tab:FSClaResults}{{IX}{15}{Fully real datasets results. Bold: the method with the highest performance excluding the upper bound (target) classifier GPC-GOLD}{table.9}{}}
\newlabel{tab:FSClaResults@cref}{{[table][9][]IX}{[1][15][]15}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{15}{section.6}\protected@file@percent }
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  A: Proof of the First Zonklar Equation}{15}{section*.1}\protected@file@percent }
\bibcite{zhang2014imbalanced}{{12}{}{{}}{{}}}
\bibcite{dawid1979maximum}{{13}{}{{}}{{}}}
\bibcite{ruiz2019learning}{{14}{}{{}}{{}}}
\bibcite{rodrigues2014gaussian}{{15}{}{{}}{{}}}
\bibcite{gonzalez2015automatic}{{16}{}{{}}{{}}}
\bibcite{rodrigues2017learning}{{17}{}{{}}{{}}}
\bibcite{rodrigues2014sequence}{{18}{}{{}}{{}}}
\bibcite{albarqouni2016aggnet}{{19}{}{{}}{{}}}
\bibcite{rodrigues2018deep}{{20}{}{{}}{{}}}
\bibcite{guan2018said}{{21}{}{{}}{{}}}
\bibcite{g2019machine}{{22}{}{{}}{{}}}
\bibcite{rodrigues2013learning}{{23}{}{{}}{{}}}
\bibcite{venanzi2014community}{{24}{}{{}}{{}}}
\bibcite{tang2019leveraging}{{25}{}{{}}{{}}}
\bibcite{zhang2011learning}{{26}{}{{}}{{}}}
\bibcite{surowiecki2005wisdom}{{27}{}{{}}{{}}}
\bibcite{hahn2018communication}{{28}{}{{}}{{}}}
\bibcite{saul2016chained}{{29}{}{{}}{{}}}
\bibcite{alvarez2012kernels}{{30}{}{{}}{{}}}
\bibcite{teh2005semiparametric}{{31}{}{{}}{{}}}
\bibcite{alvarez2010efficient}{{32}{}{{}}{{}}}
\bibcite{hoffman2013stochastic}{{33}{}{{}}{{}}}
\bibcite{yan2014learning}{{34}{}{{}}{{}}}
\bibcite{xiao2013learning}{{35}{}{{}}{{}}}
\bibcite{bishop2006pattern}{{36}{}{{}}{{}}}
\bibcite{gil2018learning}{{37}{}{{}}{{}}}
\bibcite{morales2019scalable1}{{38}{}{{}}{{}}}
\bibcite{hua2018collaborative}{{39}{}{{}}{{}}}
\bibcite{hensman2015scalable}{{40}{}{{}}{{}}}
\bibcite{moreno2018heterogeneous}{{41}{}{{}}{{}}}
\bibcite{blei2017variational}{{42}{}{{}}{{}}}
\bibcite{maaten2008visualizing}{{43}{}{{}}{{}}}
\bibcite{hernandez2020bearing}{{44}{}{{}}{{}}}
\bibcite{arias2011automatic}{{45}{}{{}}{{}}}
\bibcite{fawcett2006introduction}{{46}{}{{}}{{}}}
\bibcite{schmidhuber2015deep}{{47}{}{{}}{{}}}
\bibcite{alvarez2011computationally}{{48}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@LN@col{1}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{Biographies}{16}{IEEEbiography.0}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Michael Shell}{16}{IEEEbiography.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{John Doe}{16}{IEEEbiography.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Jane Doe}{16}{IEEEbiography.3}\protected@file@percent }
